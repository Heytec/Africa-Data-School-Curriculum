{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faf05b7",
   "metadata": {},
   "source": [
    "## Table of Content \n",
    "\n",
    "To build a simple feedforward neural network using Keras, we'll first go through the data science framework steps:\n",
    "\n",
    "### 1 Simple feedforward neural networK\n",
    "* Define the problem\n",
    "* Gather the data\n",
    "* Prepare the data\n",
    "* Create the model\n",
    "* Train the model\n",
    "* Evaluate the model\n",
    "* Deploy the model (optional)\n",
    "\n",
    "### 2 Loss Functions Available in Keras\n",
    "\n",
    "### 3 Optimization\n",
    "\n",
    "### 4  Activation Functions\n",
    "\n",
    "\n",
    "### 5 Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12e255",
   "metadata": {},
   "source": [
    "In this example, we'll use the \"Students Performance in Exams\" dataset from Kaggle to predict whether a student passes or fails based on their demographic information and test scores.\n",
    "\n",
    "You can download the dataset from this link: https://www.kaggle.com/spscientist/students-performance-in-exams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea7c53",
   "metadata": {},
   "source": [
    "## Define the problem:\n",
    "\n",
    "Our goal is to create a feedforward neural network that can predict whether a student passes or fails based on their demographic information and test scores.\n",
    "\n",
    "## Gather the data:\n",
    "\n",
    "Download the dataset from the Kaggle link provided above.\n",
    "\n",
    "## Prepare the data:\n",
    "\n",
    "First, we'll load and preprocess the data using pandas and scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8f07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/StudentsPerformance.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "data[\"average_score\"] = (data[\"math score\"] + data[\"reading score\"] + data[\"writing score\"]) / 3\n",
    "data[\"pass\"] = data[\"average_score\"] >= 60\n",
    "data[\"pass\"] = data[\"pass\"].astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "cat_cols = [\"gender\", \"race/ethnicity\", \"parental level of education\", \"lunch\", \"test preparation course\"]\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_features = one_hot_encoder.fit_transform(data[cat_cols]).toarray()\n",
    "\n",
    "# Combine encoded features with numeric features\n",
    "numeric_cols = [\"math score\", \"reading score\", \"writing score\", \"average_score\"]\n",
    "X = pd.concat([pd.DataFrame(encoded_features), data[numeric_cols].reset_index(drop=True)], axis=1)\n",
    "y = data[\"pass\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47e697",
   "metadata": {},
   "source": [
    "## Create the model:\n",
    "Now, we'll build a simple feedforward neural network using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed263b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d829d4",
   "metadata": {},
   "source": [
    "## Train the model:\n",
    "Train the neural network on the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b965ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.6158 - accuracy: 0.6625 - val_loss: 0.4786 - val_accuracy: 0.8062\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.3956 - accuracy: 0.8781 - val_loss: 0.3358 - val_accuracy: 0.8875\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.9375 - val_loss: 0.2514 - val_accuracy: 0.9250\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1968 - accuracy: 0.9516 - val_loss: 0.1953 - val_accuracy: 0.9375\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9625 - val_loss: 0.1556 - val_accuracy: 0.9563\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9703 - val_loss: 0.1297 - val_accuracy: 0.9688\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9781 - val_loss: 0.1084 - val_accuracy: 0.9688\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9844 - val_loss: 0.0949 - val_accuracy: 0.9750\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0624 - accuracy: 0.9875 - val_loss: 0.0824 - val_accuracy: 0.9750\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9891 - val_loss: 0.0795 - val_accuracy: 0.9750\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9906 - val_loss: 0.0696 - val_accuracy: 0.9750\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9953 - val_loss: 0.0644 - val_accuracy: 0.9750\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9969 - val_loss: 0.0619 - val_accuracy: 0.9750\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9969 - val_loss: 0.0583 - val_accuracy: 0.9812\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9984 - val_loss: 0.0567 - val_accuracy: 0.9750\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9812\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.9984 - val_loss: 0.0549 - val_accuracy: 0.9812\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.9984 - val_loss: 0.0559 - val_accuracy: 0.9812\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9984 - val_loss: 0.0525 - val_accuracy: 0.9812\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9750\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9812\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9812\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9812\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9812\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9812\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9812\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9812\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9812\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9812\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9812\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9812\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9812\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9812\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0553 - val_accuracy: 0.9812\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9812\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9812\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9812\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9812\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9812\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9812\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9812\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 0.9812\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9812\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9812\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9812\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9812\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 0.9812\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9812\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9812\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9812\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9812\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9812\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9812\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 0.9812\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 0.9812\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9812\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9812\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9812\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9812\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9812\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9812\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9812\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9812\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9812\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9812\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9812\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9812\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9812\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9812\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9812\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9812\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9812\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.9812\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9812\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9812\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9812\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 0.9812\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9812\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9812\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9812\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9812\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9812\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 9.7465e-04 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9812\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9812\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9812\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 9.7638e-04 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9812\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9812\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 8.2562e-04 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9812\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9812\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.6513e-04 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9812\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 8.5929e-04 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9812\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 9.2730e-04 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9812\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 8.1180e-04 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9812\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.8784e-04 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9812\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 8.4806e-04 - accuracy: 1.0000 - val_loss: 0.0658 - val_accuracy: 0.9812\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9812\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 6.0640e-04 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9812\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f1b18",
   "metadata": {},
   "source": [
    "## Evaluate the model:\n",
    "Evaluate the model's performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976cef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9750\n",
      "Test loss: 0.06563767045736313, Test accuracy: 0.9750000238418579\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9d44d",
   "metadata": {},
   "source": [
    "## Model optimization:\n",
    "\n",
    "You can optimize the model by tuning hyperparameters like the number of layers, number of neurons, learning rate, batch size, and number of epochs. To find the optimal hyperparameters, you can use techniques like Grid Search or Random Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49d7a9",
   "metadata": {},
   "source": [
    "* To demonstrate model optimization using Grid Search, we'll use the GridSearchCV class from scikit-learn with the KerasClassifier wrapper from the Keras library. In this example, we'll optimize the number of neurons in the hidden layers, batch size, and number of epochs.\n",
    "\n",
    "First, create a function that returns the Keras model with variable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66c2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model(neurons_1=32, neurons_2=16):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_1, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "    model.add(Dense(neurons_2, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032dd41",
   "metadata": {},
   "source": [
    "Now, wrap the function with KerasClassifier and set up the parameter grid for Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92eb383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:4: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "param_grid = {\n",
    "    'neurons_1': [32, 64, 128],\n",
    "    'neurons_2': [16, 32, 64],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af882070",
   "metadata": {},
   "source": [
    "Finally, fit the grid search to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95505cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "grid_result = grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32331763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3da455",
   "metadata": {},
   "source": [
    "## Save the model:\n",
    "Save the trained model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"student_pass_predictor.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d631094",
   "metadata": {},
   "source": [
    "## Metric evaluation:\n",
    "\n",
    "To further evaluate the model, you can analyze the training history to plot accuracy and loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d701b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75614a73",
   "metadata": {},
   "source": [
    "## Loss Functions Available in Keras\n",
    "\n",
    "Keras provides a range of loss functions that can be used in various machine learning problems. The following are some of the commonly used loss functions available in Keras:\n",
    "\n",
    "### Regression Loss Functions\n",
    "\n",
    "- **Mean Squared Error (MSE):** This is the most common loss function for regression problems. It measures the average squared difference between the predicted values and the actual values.\n",
    "- **Mean Absolute Error (MAE):** This is a loss function that is less sensitive to outliers than MSE. It measures the average absolute difference between the predicted values and the actual values.\n",
    "- **Huber Loss:** This is a loss function that is less sensitive to outliers than MSE. It is often used for regression problems where there may be outliers in the data.\n",
    "- **Logcosh Loss:** This is a loss function that is less sensitive to outliers than MSE. It is often used for regression problems where there may be outliers in the data.\n",
    "- **Tweedie Loss:** This is a loss function that is often used for count data. It is more robust to overfitting than MSE.\n",
    "- **Poisson Loss:** This is a loss function that is often used for count data. It is more robust to overfitting than MSE.\n",
    "\n",
    "### Classification Loss Functions\n",
    "\n",
    "- **Categorical Crossentropy (CCE):** This is the most common loss function for classification problems. It measures the difference between the predicted probabilities and the actual probabilities.\n",
    "- **Sparse Categorical Crossentropy:** This is a loss function that is similar to CCE, but it is used for sparse classification problems.\n",
    "- **Binary Crossentropy (BCE):** This is a loss function that is similar to CCE, but it is used for binary classification problems.\n",
    "- **Weighted Categorical Crossentropy:** This is a loss function that is similar to CCE, but it allows you to weight the different classes differently. This can be useful if some classes are more important than others.\n",
    "- **KL Divergence:** This is a loss function that measures the difference between two probability distributions. It is often used for natural language processing tasks.\n",
    "\n",
    "### Other Loss Functions\n",
    "\n",
    "- **Hinge Loss:** This is a loss function that is often used for binary classification problems. It measures the distance between the predicted value and the decision boundary.\n",
    "- **Triplet Loss:** This is a loss function that is often used for face recognition tasks. It measures the distance between a pair of images, where one image is the anchor image and the other image is either a positive or negative example.\n",
    "- **Custom Loss Function:** You can also create your own custom loss function. To do this, you need to create a function that takes the predicted values and the actual values as input and returns a single number as the loss.\n",
    "\n",
    "The choice of loss function depends on the type of problem that you are trying to solve. For example, if you are trying to predict a continuous value, then you would use a regression loss function such as MSE. If you are trying to classify an observation into one of a set of discrete categories, then you would use a classification loss function such as CCE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed61fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de74d4bf",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe2b77",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is a first-order optimization algorithm that finds the local minimum of the loss function by iteratively updating the weights in the direction of the negative gradient. It uses the same learning rate for all parameters and can be slow to converge.\n",
    "\n",
    "**Usage:** Linear regression, logistic regression, shallow networks\n",
    "\n",
    "**Variants:** Batch Gradient Descent, Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Momentum is a modification of gradient descent that incorporates a momentum term to speed up convergence. It helps the optimizer navigate through local minima and plateaus more effectively by adding a fraction of the previous update to the current update.\n",
    "\n",
    "**Usage:** Deep networks, CNNs, Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**Formula:** v_t = γv_(t-1) + η∇L(w_(t-1)) and w_t = w_(t-1) - v_t, where v is the momentum term, γ is the momentum coefficient, η is the learning rate, and ∇L is the gradient of the loss function\n",
    "\n",
    "### Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "NAG is a modification of momentum that incorporates the Nesterov momentum term. It improves upon the momentum method by using the lookahead gradient, which can result in better convergence.\n",
    "\n",
    "**Usage:** Deep networks, CNNs, RNNs\n",
    "\n",
    "**Formula:** v_t = γv_(t-1) + η∇L(w_(t-1) - γv_(t-1)) and w_t = w_(t-1) - v_t\n",
    "\n",
    "### Adagrad\n",
    "\n",
    "Adagrad is an adaptive learning rate optimizer that adjusts the learning rate for each parameter based on the past gradients. It is well-suited for sparse data and can converge more quickly than gradient descent.\n",
    "\n",
    "**Usage:** Deep networks, natural language processing tasks, recommendation systems\n",
    "\n",
    "**Formula:** G_t = G_(t-1) + (∇L(w_(t-1)))^2 and w_t = w_(t-1) - η * ∇L(w_(t-1)) / √(G_t + ε), where G is the sum of the squared gradients, and ε is a small constant to avoid division by zero\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "RMSprop is an adaptive learning rate optimizer that improves upon Adagrad by using an exponentially decaying average of the squared gradients. This helps prevent the learning rate from becoming too small, which can happen with Adagrad.\n",
    "\n",
    "**Usage:** Deep networks, CNNs, RNNs\n",
    "\n",
    "**Formula:** G_t = βG_(t-1) + (1-β)(∇L(w_(t-1)))^2 and w_t = w_(t-1) - η * ∇L(w_(t-1)) / √(G_t + ε), where β is the decay rate\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam is an adaptive learning rate optimizer that combines elements of both momentum and RMSprop. It maintains separate learning rates for each parameter and adapts them based on the first and second moments of the gradients.\n",
    "\n",
    "**Usage:** Deep networks, CNNs, RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12555adf",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into a neural network, allowing it to learn complex patterns and make predictions based on the input data. They are applied to the output of each neuron in the network. Here are some common activation functions and their typical use cases:\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "The sigmoid function maps input values to the range (0, 1), making it suitable for binary classification tasks or any task requiring an output in the probability range.\n",
    "\n",
    "**Usage:** Output layer in binary classification, hidden layers in shallow networks\n",
    "\n",
    "**Formula:** sigmoid(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "### Tanh (Hyperbolic Tangent)\n",
    "\n",
    "The tanh function maps input values to the range (-1, 1). It is similar to the sigmoid function but has a wider output range, making it more suitable for hidden layers in some networks.\n",
    "\n",
    "**Usage:** Hidden layers in shallow networks\n",
    "\n",
    "**Formula:** tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU is a piecewise linear function that outputs the input value if it's positive and 0 otherwise. It is computationally efficient and helps mitigate the vanishing gradient problem in deep networks.\n",
    "\n",
    "**Usage:** Hidden layers in deep networks, convolutional neural networks (CNNs)\n",
    "\n",
    "**Formula:** ReLU(x) = max(0, x)\n",
    "\n",
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is a variation of the ReLU function that allows a small, non-zero gradient for negative input values. This can help reduce the \"dying ReLU\" problem, where neurons become inactive and don't contribute to the learning process.\n",
    "\n",
    "**Usage:** Hidden layers in deep networks, CNNs\n",
    "\n",
    "**Formula:** LeakyReLU(x) = max(αx, x), where α is a small constant (e.g., 0.01)\n",
    "\n",
    "### ELU (Exponential Linear Unit)\n",
    "\n",
    "ELU is another variation of the ReLU function that smooths the curve for negative input values, allowing for non-zero gradients. This can help with the vanishing gradient problem.\n",
    "\n",
    "**Usage:** Hidden layers in deep networks, CNNs\n",
    "\n",
    "**Formula:** ELU(x) = x if x > 0, else α(exp(x) - 1), where α is a constant (e.g., 1)\n",
    "\n",
    "### Softmax\n",
    "\n",
    "The softmax function is used in multi-class classification tasks. It maps input values to a probability distribution over multiple classes, ensuring that the sum of probabilities for all classes equals 1.\n",
    "\n",
    "**Usage:** Output layer in multi-class classification\n",
    "\n",
    "**Formula:** softmax(x)_i = exp(x_i) / Σ(exp(x_j)) for all j\n",
    "\n",
    "Choosing the right activation function depends on the problem you're trying to solve, the architecture of your neural network, and the input data's nature. Different activation functions have different properties, and using the appropriate activation function can improve the model's performance and training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222ce95",
   "metadata": {},
   "source": [
    "# Assignment:\n",
    "\n",
    "Predicting House Prices Using a Feedforward Neural Network\n",
    "\n",
    "### Objective: \n",
    "Create a feedforward neural network to predict house prices based on various features such as the number of rooms, location, size, and other factors.\n",
    "\n",
    "### Dataset:\n",
    "Use the \"House Prices: Advanced Regression Techniques\" dataset available on Kaggle. You can download the dataset from this link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "* Define the problem: Determine the goal of the project and the problem you're trying to solve using the feedforward neural network.\n",
    "\n",
    "* Gather the data: Download the dataset from the Kaggle link provided above.\n",
    "\n",
    "* Prepare the data: Clean and preprocess the data. Handle missing values, encode categorical variables, and normalize or standardize numerical variables.\n",
    "\n",
    "* Create the model: Build a feedforward neural network using Keras with an appropriate architecture for predicting house prices. You can experiment with different numbers of layers, neurons, and activation functions.\n",
    "\n",
    "* Train the model: Train the neural network on the preprocessed data. Experiment with different batch sizes and numbers of epochs to improve the model's performance.\n",
    "\n",
    "* Evaluate the model: Test the model's performance on a separate validation or test set. Calculate metrics such as mean squared error, mean absolute error, and R-squared to assess the model's accuracy.\n",
    "\n",
    "* Optimize the model: Perform hyperparameter tuning to find the optimal architecture and parameters for the neural network. You can use techniques like Grid Search or Random Search.\n",
    "\n",
    "Present your findings: Create a report or presentation summarizing your methodology, model architecture, performance metrics, and any insights or recommendations based on your findings.\n",
    "\n",
    "This assignment will help you gain a deeper understanding of feedforward neural networks and their applications in regression tasks. Additionally, you'll gain experience in data preprocessing, model evaluation, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cd4eb",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"data/house/train.csv\")\n",
    "test_data = pd.read_csv(\"data/house/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b95e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step - loss: 39030476800.0000\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 38830428160.0000\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 37231734784.0000\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 31161845760.0000\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 19147220992.0000\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 8795296768.0000\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 5417553920.0000\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 3793114880.0000\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 2825304320.0000\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 2314813696.0000\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 2027522944.0000\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1835650048.0000\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1685429120.0000\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1562385408.0000\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1455504256.0000\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1360538880.0000\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 1282001152.0000\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1210584832.0000\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1149873280.0000\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1095772288.0000\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 1043565568.0000\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 994519232.0000\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 955071552.0000\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 916668672.0000\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 882703232.0000\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 852935040.0000\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 821625728.0000\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 793678016.0000\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 769010304.0000\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 744497152.0000\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 722378752.0000\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 703705024.0000\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 683358400.0000\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 667935360.0000\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 647680512.0000\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 629504128.0000\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 614188608.0000\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 598399040.0000\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 585787904.0000\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 570677312.0000\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 560319424.0000\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 547842816.0000\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 538078272.0000\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 526026720.0000\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 514631584.0000\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 501288544.0000\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 492382432.0000\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 481750560.0000\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 471860544.0000\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 461202496.0000\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 455215392.0000\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 446716896.0000\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 437147392.0000\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 429284896.0000\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 424082240.0000\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 413354720.0000\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 407657760.0000\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 399550848.0000\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 391483520.0000\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 389365568.0000\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 379497248.0000\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 374989312.0000\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 366892640.0000\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 363845568.0000\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 358568544.0000\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 350209152.0000\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 347301152.0000\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 341773024.0000\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 337158560.0000\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 331353600.0000\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 338883872.0000\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 324528128.0000\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 318623936.0000\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 316099712.0000\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 309777792.0000\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 308024736.0000\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 304749312.0000\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 301937152.0000\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 296383072.0000\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 293389696.0000\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 291288000.0000\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 286154880.0000\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 282681120.0000\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 281192672.0000\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 275775904.0000\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 272785216.0000\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 273500192.0000\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 272962432.0000\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 264274976.0000\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 262397264.0000\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 257855696.0000\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 255167200.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 255753520.0000\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 251221120.0000\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 247949392.0000\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 243718416.0000\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 242654048.0000\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 240876944.0000\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 238526704.0000\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 235969376.0000\n",
      "46/46 [==============================] - 0s 2ms/step\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 218241680.0000\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 213170112.0000\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 208837840.0000\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 206006928.0000\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 202451536.0000\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 199937504.0000\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 198235200.0000\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 194529088.0000\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 192765552.0000\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 191307856.0000\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 190033168.0000\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 187256304.0000\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 185169376.0000\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 183563776.0000\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 181511664.0000\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 179120544.0000\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 178876496.0000\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 177223776.0000\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 174868336.0000\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 171901360.0000\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 172785232.0000\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 169940320.0000\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 168634608.0000\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 166311088.0000\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 165397152.0000\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 167034752.0000\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 162920848.0000\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 160594208.0000\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 161803088.0000\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 158154528.0000\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 157056352.0000\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 155134576.0000\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 154172128.0000\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 155858416.0000\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 160455856.0000\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 151345088.0000\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 148501456.0000\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 147308944.0000\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 147134048.0000\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 144013536.0000\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 146760304.0000\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 143239392.0000\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 142843504.0000\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 140722608.0000\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 138733904.0000\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 139762512.0000\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 135749808.0000\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 135389504.0000\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 141134672.0000\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 132381696.0000\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 132945208.0000\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 132992880.0000\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 129768536.0000\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 127358712.0000\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 126368248.0000\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 127160416.0000\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 125772832.0000\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 125555448.0000\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 123121864.0000\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 122288760.0000\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 121038288.0000\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 120922144.0000\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 119212312.0000\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 118738928.0000\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 120500896.0000\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 114531864.0000\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 116443488.0000\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 113993584.0000\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 112466248.0000\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 113418352.0000\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 108711816.0000\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 111826496.0000\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 108311176.0000\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 107833152.0000\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 108016112.0000\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 106067744.0000\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 105524984.0000\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 104653512.0000\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 104200072.0000\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 103705008.0000\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 102445184.0000\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 100431432.0000\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 99852760.0000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 97576688.0000\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 98279040.0000\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 96972992.0000\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 97720928.0000\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 96008536.0000\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 94176344.0000\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 94725288.0000\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 93428440.0000\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 92002096.0000\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 92629528.0000\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 92987048.0000\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 90859704.0000\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 89897072.0000\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 88485832.0000\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 86481192.0000\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 85775272.0000\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 85925336.0000\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 504796752.64394903, Mean Absolute Error: 14977.515410958904, R-squared: 0.9341883592680905\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"data/house/train.csv\")\n",
    "test_data = pd.read_csv(\"data/house/test.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "train_data.fillna(-1, inplace=True)\n",
    "test_data.fillna(-1, inplace=True)\n",
    "\n",
    "# Convert categorical features to strings\n",
    "cat_cols = train_data.select_dtypes(include=['object']).columns\n",
    "train_data[cat_cols] = train_data[cat_cols].astype(str)\n",
    "test_data[cat_cols] = test_data[cat_cols].astype(str)\n",
    "\n",
    "# Encode categorical features\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_features_train = one_hot_encoder.fit_transform(train_data[cat_cols]).toarray()\n",
    "encoded_features_test = one_hot_encoder.transform(test_data[cat_cols]).toarray()\n",
    "\n",
    "# Combine encoded features with numeric features\n",
    "numeric_cols = train_data.select_dtypes(include=['int64', 'float64']).drop(['Id', 'SalePrice'], axis=1).columns\n",
    "X_train = pd.concat([pd.DataFrame(encoded_features_train), train_data[numeric_cols].reset_index(drop=True)], axis=1)\n",
    "X_test = pd.concat([pd.DataFrame(encoded_features_test), test_data[numeric_cols].reset_index(drop=True)], axis=1)\n",
    "y_train = train_data['SalePrice']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optional: evaluate the model using a separate validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}, Mean Absolute Error: {mae}, R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaec3d",
   "metadata": {},
   "source": [
    "* Mean Squared Error (MSE): MSE is a measure of the average squared difference between the predicted values and the actual values. In this case, the MSE is 504796752.64394903, which means that the average squared difference between the predicted values and the actual values is 504796752.64394903.\n",
    "* Mean Absolute Error (MAE): MAE is a measure of the average absolute difference between the predicted values and the actual values. In this case, the MAE is 14977.515410958904, which means that the average absolute difference between the predicted values and the actual values is 14977.515410958904.\n",
    "* R-squared : R-squared is a measure of how well the model fits the data. It is a number between 0 and 1, where 0 means that the model does not fit the data at all and 1 means that the model fits the data perfectly. In this case, the R-squared is 0.9341883592680905, which means that the model fits the data very well.\n",
    "\n",
    "    In general, a lower MSE and MAE indicate better model accuracy. A higher R-squared indicates a better fit between the model's predictions and the actual observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde967f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
